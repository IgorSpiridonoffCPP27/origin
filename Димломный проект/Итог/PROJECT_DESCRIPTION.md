# Описание проекта "Краулер для анализа веб-страниц"

## Общее описание
Проект представляет собой систему для краулинга веб-страниц, анализа текстового содержимого и индексации слов в базе данных PostgreSQL. Система состоит из трех основных компонентов: краулера, сервера API и веб-прокси.

## Архитектура проекта

### Основные компоненты:
1. **crowler** - основной краулер для обхода веб-страниц
2. **server** - сервер API для работы с базой данных
3. **web_server** - веб-прокси с генерацией HTML

## Структура файлов

### Заголовочные файлы (include/)

#### `pch.h` - Предкомпилированный заголовок
- Содержит все основные включения библиотек
- Boost.Asio для асинхронного I/O
- Boost.Beast для HTTP/HTTPS
- PostgreSQL (libpqxx) для работы с БД
- OpenSSL для SSL/TLS
- ICU для работы с Unicode

#### `crawler.h` - Основной класс краулера
- **Класс Crawler**: основной функционал для обхода веб-страниц
- **Методы**:
  - `start()`, `stop()` - управление краулером
  - `crawl_url()`, `crawl_start_url()` - обход страниц
  - `process_word()` - обработка слов
  - `download_page()` - загрузка страниц
- **Приватные методы** для рекурсивного обхода и анализа

#### `DBusers.h` - Работа с базой данных
- **Класс DBuse**: управление подключениями к PostgreSQL
- **Функционал**:
  - Пул соединений с БД
  - CRUD операции для слов и URL
  - Аутентификация пользователей
  - Миграции базы данных
- **Методы**:
  - `process_word_request()` - обработка запросов слов
  - `save_word_url()` - сохранение связей слов с URL
  - `get_tables()`, `get_columns()` - метаданные БД

#### `crawler_config.h` - Конфигурация краулера
- **Класс CrawlerConfig**: парсинг config.ini
- **Параметры**: глубина рекурсии, лимиты соединений, интервалы

#### `html_parser.h` - Парсинг HTML
- **Функции** для извлечения текста из HTML с помощью Gumbo
- Очистка HTML от тегов и скриптов

#### `threadsafe_set.h` - Потокобезопасное множество
- **Класс ThreadSafeSet**: синхронизированное хранение URL

#### `url_tools.h` - Утилиты для работы с URL
- **Функции**:
  - `url_encode()` - кодирование URL
  - `normalize_url()` - нормализация URL
  - `get_host()` - извлечение хоста

#### `word_tools.h` - Обработка слов
- **Функция** `filter_and_normalize_word()` - фильтрация и нормализация слов

#### `logger.h` - Система логирования
- **Макрос LOG()** для записи сообщений в файл

#### `config_parser.h` - Парсер конфигурации
- **Класс ConfigParser**: чтение и парсинг INI файлов

#### `api_client.h` - HTTP клиент для API
- **Класс ApiClient**: отправка HTTP запросов

#### `html_generator.h` - Генерация HTML
- **Функции** для создания HTML страниц

#### `proxy_config.h` - Конфигурация прокси
- **Класс ProxyConfig**: настройки веб-прокси

### Исходные файлы (src/)

#### `crowler.cpp` - Главная программа краулера
- **main()**: инициализация, запуск краулера, интерактивный режим
- Обработка сигналов и исключений
- Цикл повторного анализа по запросу пользователя

#### `crawler.cpp` - Реализация краулера
- **Класс Crawler**: полная реализация всех методов
- Асинхронная загрузка страниц через Boost.Asio
- Парсинг HTML и извлечение слов
- Сохранение данных в БД
- Рекурсивный обход страниц

#### `DBusers.cpp` - Реализация работы с БД
- **Класс DBuse**: все методы для работы с PostgreSQL
- Управление пулом соединений
- SQL запросы и транзакции
- Обработка ошибок БД

#### `server.cpp` - API сервер
- HTTP сервер для обработки запросов к БД
- REST API для работы со словами и URL

#### `web_proxy.cpp` - Веб-прокси
- Прокси-сервер с генерацией HTML страниц
- Интеграция с API клиентом

#### `html_parser.cpp` - Парсинг HTML
- Использование Gumbo для извлечения текста
- Очистка HTML от разметки

#### `word_tools.cpp` - Обработка слов
- Фильтрация слов по длине и символам
- Нормализация (приведение к нижнему регистру)

#### `url_tools.cpp` - Утилиты URL
- Кодирование и нормализация URL
- Извлечение компонентов URL

#### `threadsafe_set.cpp` - Потокобезопасное множество
- Реализация синхронизированного хранения

#### `logger.cpp` - Логирование
- Запись сообщений в файл с временными метками

#### `crawler_config.cpp` - Конфигурация
- Парсинг config.ini файла

#### `api_client.cpp` - HTTP клиент
- Отправка HTTP запросов к API

#### `html_generator.cpp` - Генерация HTML
- Создание HTML страниц для веб-интерфейса

#### `proxy_config.cpp` - Конфигурация прокси
- Настройки веб-прокси сервера

### Конфигурационные файлы

#### `config.ini` - Основная конфигурация
- **Database**: настройки PostgreSQL
- **Crowler**: параметры краулера (глубина, лимиты)
- **web_proxy**: настройки веб-сервера
- **server**: порт API сервера
- **DBusers**: размер пула соединений

#### `CMakeLists.txt` - Система сборки
- Настройка зависимостей (Boost, OpenSSL, PostgreSQL, Gumbo)
- Сборка трех исполняемых файлов
- Настройка компилятора MSVC

## Принцип работы

### 1. Краулер (crowler)
1. Читает конфигурацию из config.ini
2. Подключается к PostgreSQL и создает таблицы
3. Начинает обход стартовой страницы
4. Рекурсивно обходит связанные страницы
5. Извлекает текст из HTML и разбивает на слова
6. Сохраняет слова и их связи с URL в БД
7. Предлагает повторный анализ

### 2. API Сервер (server)
1. Запускает HTTP сервер на указанном порту
2. Обрабатывает REST запросы к базе данных
3. Предоставляет API для работы со словами и URL

### 3. Веб-прокси (web_server)
1. Работает как прокси-сервер
2. Генерирует HTML страницы для веб-интерфейса
3. Интегрируется с API сервером

## Технологии

- **C++20** - основной язык
- **Boost.Asio** - асинхронный I/O
- **Boost.Beast** - HTTP/HTTPS
- **PostgreSQL** - база данных
- **Gumbo** - парсинг HTML
- **OpenSSL** - SSL/TLS
- **ICU** - Unicode поддержка
- **CMake** - система сборки

## Зависимости

- Boost 1.70+ (system, coroutine, context, date_time)
- OpenSSL
- libpqxx (PostgreSQL)
- Gumbo (HTML парсер)
- ICU (Unicode)
- nlohmann/json (JSON)

## Сборка

```bash
mkdir build
cd build
cmake ..
cmake --build . --config Release
```

## Использование

1. Настроить config.ini (база данных, стартовая страница)
2. Запустить crowler.exe для анализа
3. Запустить server.exe для API
4. Запустить web_server.exe для веб-интерфейса
